{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabalho Prático 3\n",
    "\n",
    "**Nome:** Vítor L. G. Silva <br>\n",
    "\n",
    "**Matrícula:** 3045"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "        \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "        \n",
    "input_path = Path('/kaggle/input/tabular-playground-series-mar-2021/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leitura dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(input_path / 'train.csv', index_col='id')\n",
    "display(train.head())\n",
    "\n",
    "train.to_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(input_path / 'test.csv', index_col='id')\n",
    "display(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(input_path / 'sample_submission.csv', index_col='id')\n",
    "display(submission.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codificação das categorias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in train.columns:\n",
    "    if train[c].dtype=='object': \n",
    "        lbl = LabelEncoder()\n",
    "        lbl.fit(list(train[c].values) + list(test[c].values))\n",
    "        train[c] = lbl.transform(train[c].values)\n",
    "        test[c] = lbl.transform(test[c].values)\n",
    "        \n",
    "display(train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull out the target, and make a validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = train.pop('target')\n",
    "X_train, X_test, y_train, y_test = train_test_split(train, target, train_size=0.60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Random Forest\n",
    "\n",
    "In previous Tabular Playground Series competition, when the target was continuous, we created a \"naive\" dummy model, that just predicted the average of the target. That approach is less useful when the scoring metric is AUC, since any constant prediction will score 0.5. So we'll skip that this time, and note that we want to score better than 0.5 for our model to be considered better than naive or random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=200, max_depth=7, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict_proba(X_test)[:, 1] # This grabs the positive class prediction\n",
    "score = roc_auc_score(y_test, y_pred)\n",
    "print(f'{score:0.5f}') # 0.87323 shows we're doing better than a dummy model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import Binarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset sem a coluna 'target'\n",
    "X_target = train\n",
    "# # Dataset apenas com a coluna 'target'\n",
    "Y_target = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Instanciar o modelo\n",
    "model = tf.keras.models.Sequential(name = 'Sequential_Model')\n",
    "\n",
    "## 2. Adicionar as camadas\n",
    "model.add(tf.keras.layers.Dense(units = 8,\n",
    "                                activation = 'relu',\n",
    "                                input_shape = (X_target.shape[1], ),\n",
    "                                name = 'input'))\n",
    "model.add(tf.keras.layers.Dense(units = 1,\n",
    "                                activation = 'linear',\n",
    "                                name = 'output'))\n",
    "\n",
    "## 3. Compilar\n",
    "model.compile(optimizer = 'adam', loss = 'mse')\n",
    "print(model.summary())\n",
    "\n",
    "## 4. Fit\n",
    "model.fit(X_target, Y_target, epochs = 1, batch_size = 1, validation_split=0.2, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar as predições\n",
    "yhat_survived = model.predict(df_test[useful_columns])[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformando em binário as predições obtidas\n",
    "binarizer = Binarizer(0.5)\n",
    "test_predict_result = binarizer.fit_transform(yhat_survived.reshape(-1, 1))\n",
    "test_predict_result = test_predict_result.astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's take a look at how the model predicted the various classes\n",
    "\n",
    "The graph below shows that the model does well with most of the negative observations, but struggles with many of the positive observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "plt.hist(y_pred[np.where(y_test == 0)], bins=100, alpha=0.75, label='neg class')\n",
    "plt.hist(y_pred[np.where(y_test == 1)], bins=100, alpha=0.75, label='pos class')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's train it on all the data and make a submission!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=200, max_depth=7, n_jobs=-1)\n",
    "clf.fit(train, target)\n",
    "submission['target'] = clf.predict_proba(test)[:, 1]\n",
    "submission.to_csv('random_forest.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now you should save your Notebook (blue button in the upper right), and then when that's complete go to the notebook viewer and make a submission to the competition. :-)\n",
    "\n",
    "## There's lots of room for improvement. What things can you try to get a better score?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
